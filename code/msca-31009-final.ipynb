{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# use pillow for checking image sizes\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Image Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to identify all unique image sizes for images in a directory\n",
    "def print_img_sizes(path):\n",
    "    # Get a list of all image file names in the directory\n",
    "    image_files = [file for file in os.listdir(path) if file.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    # Create a set to store unique image sizes\n",
    "    unique_sizes = set()\n",
    "\n",
    "    # Iterate over each image file and store its size in the set\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(path, image_file)\n",
    "        with Image.open(image_path) as image:\n",
    "            width, height = image.size\n",
    "            size = (width, height)\n",
    "            unique_sizes.add(size)\n",
    "\n",
    "    # Print the unique image sizes\n",
    "    print(\"Unique Image Sizes:\")\n",
    "    for size in unique_sizes:\n",
    "        print(f\"{size[0]}x{size[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Image Sizes:\n",
      "32x32\n"
     ]
    }
   ],
   "source": [
    "print_img_sizes('../data/test/FAKE/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images are 32x32, and are in color."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build ImageDataGenerator Classes for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an instance of the ImageDataGenerator class\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# build the training set\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    directory=\"../data/train\",\n",
    "    target_size=(32,32),\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape of each observation:  (32, 32, 3)\n",
      "Number of classes:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Image shape of each observation: \",train_generator.image_shape)\n",
    "print(\"Number of classes: \",train_generator.num_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build ImageDataGenerator Class for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generate the test set\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    directory=\"../data/test\",\n",
    "    target_size=(32,32),\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False # Make sure this is false so that predictions will align w correct image labels later on\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model as a function so that we can perform hyperparameter tuning\n",
    "def create_model(filters=32,kernel_size=(3,3), pool_size=(2,2), hidden_units=128, dropout_rate=.5, learning_rate=0.001):\n",
    "\n",
    "    # Create an instance of Sequential\n",
    "    classifier = Sequential()\n",
    "\n",
    "    # Add a Conv2D layer. Applies a set of filters to the input data, each filter learns to recognize different patterns or features\n",
    "    classifier.add(Conv2D(filters=filters, \n",
    "                        kernel_size=kernel_size, \n",
    "                        input_shape=train_generator.image_shape, \n",
    "                        activation='relu')\n",
    "                        )\n",
    "\n",
    "    # Add a MaxPooling2d layer. Performs downsampling on the data, reduces dimensions. Divides input data into non-overlapping regions (pooling windows).\n",
    "    # Maximum value is output within each window.\n",
    "    classifier.add(MaxPooling2D(pool_size))\n",
    "\n",
    "    classifier.add(Conv2D(filters=filters, \n",
    "                        kernel_size=kernel_size, \n",
    "                        input_shape=train_generator.image_shape, \n",
    "                        activation='relu')\n",
    "                        )\n",
    "    \n",
    "    classifier.add(MaxPooling2D(pool_size))\n",
    "\n",
    "    # Add a Flatten layer. Reshape data into a 1d array. Transition the convolution and pooling layers to the fully connected layers.\n",
    "    classifier.add(Flatten())\n",
    "\n",
    "    # Add a Dense layer. A fully connected layer, allows for the learning of relationships. Activation function introduces non-linearity\n",
    "    classifier.add(Dense(hidden_units,\n",
    "                        activation='relu')\n",
    "                        )\n",
    "    \n",
    "    classifier.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Add a Final Dense layer. This will output our probabilities.\n",
    "    classifier.add(Dense(units=train_generator.num_classes,\n",
    "                        activation='sigmoid')\n",
    "                        )\n",
    "\n",
    "    # Compile the model\n",
    "    classifier.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "64/64 [==============================] - 3s 13ms/step - loss: 0.6497 - accuracy: 0.6235\n",
      "Epoch 2/5\n",
      "64/64 [==============================] - 1s 14ms/step - loss: 0.5607 - accuracy: 0.7222\n",
      "Epoch 3/5\n",
      "64/64 [==============================] - 1s 13ms/step - loss: 0.5352 - accuracy: 0.7344\n",
      "Epoch 4/5\n",
      "64/64 [==============================] - 1s 13ms/step - loss: 0.5037 - accuracy: 0.7466\n",
      "Epoch 5/5\n",
      "64/64 [==============================] - 1s 13ms/step - loss: 0.5142 - accuracy: 0.7490\n",
      "Model saved to disk\n"
     ]
    }
   ],
   "source": [
    " # Create and compile a model with the selected hyperparameters\n",
    "model = create_model(filters=64,kernel_size=(3,3), pool_size=(3,3), hidden_units=256, dropout_rate=.2, learning_rate=0.001)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, epochs=5, batch_size=32, steps_per_epoch=64, verbose=1)\n",
    "\n",
    "# save the model to disk\n",
    "model.save('mod_1.h5')\n",
    "print('Model saved to disk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating model performance and generating predictions\n",
    "def evaluate_model(path_to_mod, generator):\n",
    "    # Load the trained model\n",
    "    model = keras.models.load_model(path_to_mod)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions = model.predict(generator)\n",
    "\n",
    "    # Convert the predictions to class labels\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Get the true class labels\n",
    "    true_classes = generator.classes\n",
    "\n",
    "    accuracy = accuracy_score(predicted_classes, true_classes)\n",
    "    print('Prediction Accuracy: ', accuracy)\n",
    "\n",
    "    return predicted_classes,true_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 8s 13ms/step\n",
      "Prediction Accuracy:  0.76915\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.78      0.77      9727\n",
      "           1       0.78      0.76      0.77     10273\n",
      "\n",
      "    accuracy                           0.77     20000\n",
      "   macro avg       0.77      0.77      0.77     20000\n",
      "weighted avg       0.77      0.77      0.77     20000\n",
      "\n",
      "Class Labels:  {'FAKE': 0, 'REAL': 1}\n"
     ]
    }
   ],
   "source": [
    "preds, actual = evaluate_model('mod_1.h5', test_generator)\n",
    "print(classification_report(preds, actual))\n",
    "print(\"Class Labels: \", test_generator.class_indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Grid Search\n",
    "\n",
    "Performing a random grid search on a number of hyper parameters will assist us with fune tuing our model for the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filters': 24, 'kernel_size': (2, 2), 'pool_size': (2, 2), 'hidden_units': 256, 'dropout_rate': 0.2, 'learning_rate': 0.001}\n",
      "Model saved to disk\n",
      "625/625 [==============================] - 12s 18ms/step\n",
      "Prediction Accuracy:  0.80375\n",
      "{'filters': 32, 'kernel_size': (3, 3), 'pool_size': (3, 3), 'hidden_units': 256, 'dropout_rate': 0.2, 'learning_rate': 0.001}\n",
      "Model saved to disk\n",
      "625/625 [==============================] - 15s 24ms/step\n",
      "Prediction Accuracy:  0.7396\n"
     ]
    }
   ],
   "source": [
    "# Define a hyper parameter grid\n",
    "param_grid = {\n",
    "    'filters': [16, 24, 32, 64, 128, 256],\n",
    "    'kernel_size': [(2, 2), (3, 3)],\n",
    "    'pool_size': [(2, 2), (3,3)],\n",
    "    'hidden_units': [64, 128, 256],\n",
    "    'dropout_rate': [0.2, 0.3, 0.5],\n",
    "    'learning_rate': [.0001, 0.001, 0.005]\n",
    "}\n",
    "\n",
    "# Perform random grid search\n",
    "num_iterations = 2\n",
    "best_accuracy = 0.0 # placeholder to track best accuracy\n",
    "best_params = None # placeholder to track best parameters\n",
    "\n",
    "# Create a dataframe to track model performance using various hyper parameters\n",
    "col_names = list(param_grid.keys()) # names of hyper parameters\n",
    "params_df = pd.DataFrame(columns=col_names)\n",
    "mod_df = pd.DataFrame(columns=['mod_name','accuracy'])\n",
    "params_df = pd.concat([params_df, mod_df], axis=1) # combine to create df for performance tracking\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # Randomly select hyperparameters from the grid\n",
    "    params = {param: random.choice(values) for param, values in param_grid.items()}\n",
    "\n",
    "    print(params)\n",
    "\n",
    "    # Create and compile the model with the selected hyperparameters\n",
    "    model = create_model(**params)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(train_generator, epochs=5, batch_size=32, steps_per_epoch=64, verbose=0)\n",
    "\n",
    "    # save the model to disk\n",
    "    mod_name = 'mod_rgs_'+str(i)+'.h5'\n",
    "    model.save(mod_name)\n",
    "    print('Model saved to disk')\n",
    "\n",
    "    preds, actual = evaluate_model(mod_name, test_generator)\n",
    "\n",
    "    accuracy = accuracy_score(preds,actual)\n",
    "\n",
    "    # add a row to our tracker dataframe\n",
    "    converted_list = [str(item) if isinstance(item, set) else item for item in list(params.values())] # convert items in a list that are a set to string\n",
    "    converted_list.extend([mod_name, accuracy])\n",
    "    params_df = pd.concat([params_df, pd.DataFrame([converted_list], columns=params_df.columns)], ignore_index=True) # add new row to tracker df\n",
    "\n",
    "    # Check if the current model outperforms the previous best model\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_params = params\n",
    "\n",
    "        model.save('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel_size</th>\n",
       "      <th>pool_size</th>\n",
       "      <th>hidden_units</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>mod_name</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>256</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>mod_rgs_0.h5</td>\n",
       "      <td>0.80375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>256</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>mod_rgs_1.h5</td>\n",
       "      <td>0.73960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filters kernel_size pool_size hidden_units  dropout_rate  learning_rate  \\\n",
       "0      24      (2, 2)    (2, 2)          256           0.2          0.001   \n",
       "1      32      (3, 3)    (3, 3)          256           0.2          0.001   \n",
       "\n",
       "       mod_name  accuracy  \n",
       "0  mod_rgs_0.h5   0.80375  \n",
       "1  mod_rgs_1.h5   0.73960  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
